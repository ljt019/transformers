use crate::Message;
use candle_core::Tensor;

/// Minimal interface required by the text-generation pipeline for a model context.
///
/// Both `Qwen3Model::Context` and `Gemma3Model::Context` already expose compatible
/// `generate` and `reset` methods, so we only need a thin trait wrapper that the
/// pipeline can work with generically.
pub trait LanguageModelContext: Send {
    /// Forward the input tokens through the model, returning the logits for the
    /// next token.
    fn generate(&mut self, input: &Tensor) -> candle_core::Result<Tensor>;

    /// Clear the internal state (kv-cache, position, etc.).
    fn reset(&mut self);

    /// Get the current position (number of cached tokens).
    fn position(&self) -> usize;

    /// Check if the cache is still valid for continuing from a given position.
    fn can_continue_from(&self, position: usize) -> bool;
}

pub trait TextGenerationModel {
    /// Type used to configure model loading (e.g. which checkpoint size).
    type Options;
    /// The context type that will be returned by `new_context` and consumed by
    /// the pipeline. It must implement [`LanguageModelContext`] and be `Send`
    /// so that asynchronous streams capturing it can be moved across threads.
    type Context: LanguageModelContext + Send;

    fn new(options: Self::Options) -> Self;

    fn get_tokenizer(&self) -> anyhow::Result<tokenizers::Tokenizer>;

    fn apply_chat_template(&self, messages: &[Message]) -> anyhow::Result<String>;

    fn get_eos_token(&self) -> u32;

    fn get_max_seq_len(&self) -> usize;

    fn new_context(&self) -> Self::Context;

    fn clear_context(&self, context: &mut Self::Context) -> anyhow::Result<()>;
}

pub trait Reasoning {}

pub trait ToggleableReasoning {
    fn set_reasoning(&mut self, enable: bool) -> anyhow::Result<()>;
}

pub trait ToolCalling {
    fn register_tool(&mut self, tool: Tool) -> anyhow::Result<()>;
    fn registered_tools(&self) -> Vec<Tool>;
    fn call_tool(
        &mut self,
        tool_name: String,
        parameters: std::collections::HashMap<String, String>,
    ) -> anyhow::Result<String>;
}

#[derive(Clone, serde::Serialize)]
#[allow(clippy::type_complexity)]
pub struct Tool {
    pub(crate) name: String,
    pub(crate) description: String,
    pub(crate) parameters: std::collections::HashMap<String, String>,
    #[serde(skip_serializing)]
    pub(crate) function: fn(parameters: std::collections::HashMap<String, String>) -> String,
}

impl Tool {
    /// Create a new tool description that can be registered with a model.
    pub fn new(
        name: String,
        description: String,
        parameters: std::collections::HashMap<String, String>,
        function: fn(parameters: std::collections::HashMap<String, String>) -> String,
    ) -> Self {
        Self {
            name,
            description,
            parameters,
            function,
        }
    }

    /// Get the tool name.
    pub fn name(&self) -> &str {
        &self.name
    }

    /// Execute the tool with the given parameters, returning its string result.
    pub fn call(&self, parameters: std::collections::HashMap<String, String>) -> String {
        (self.function)(parameters)
    }

    /// Get a reference to the declared parameters schema.
    pub fn parameters(&self) -> &std::collections::HashMap<String, String> {
        &self.parameters
    }

    /// Get the description of the tool.
    pub fn description(&self) -> &str {
        &self.description
    }
}

/// Local trait to convert various user-facing representations into a [`Tool`].
/// Having our own trait lets us implement it for function pointers generated by
/// the `#[tool]` macro without violating Rust's orphan rules.
pub trait IntoTool {
    fn into_tool(self) -> Tool;
}

impl IntoTool for Tool {
    fn into_tool(self) -> Tool {
        self
    }
}
